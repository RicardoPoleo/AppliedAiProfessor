{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-16T18:29:22.049277Z",
     "start_time": "2024-08-16T18:29:22.029947Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:29:25.975012Z",
     "start_time": "2024-08-16T18:29:24.253461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math professor\",\n",
    "    instructions=\"You are an Math professor. Use you knowledge base to answer questions about math lectures based on the provided files.\",\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ],
   "id": "fd68b7c161c51545",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:30:16.361690Z",
     "start_time": "2024-08-16T18:30:13.219233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_store = client.beta.vector_stores.create(name=\"Math 201 lecture\")\n",
    "\n",
    "# prepare the files for upload to OpenAI\n",
    "file_paths = [\"C:\\projects\\Sports-Buddy\\support_material\\Lecture01_Script.pdf\",\n",
    "              \"C:\\projects\\Sports-Buddy\\support_material\\Lecture02_Script.pdf\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store, and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ],
   "id": "52b170ad41e539b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=2, failed=0, in_progress=0, total=2)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:30:27.049011Z",
     "start_time": "2024-08-16T18:30:22.397146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        print(\"\\n\".join(citations))\n",
    "\n",
    "\n",
    "student_question = \"Can you explain what functions are?\"\n",
    "\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": student_question}],\n",
    "    tool_resources={\n",
    "        \"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store.id]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "current_lecture = \"Lecture 1\"\n",
    "instruction_for_search = \"Search in which lecture the requested concept is explained. If it appears on another Lecture that in not the current lecture, reply explaining that concept is out of the scope of this class (in a friendly manner), since it will be explained in the lecture X, where X is the lecture where it appears.\"\n",
    "length_limitation = \"Keep your answers short, no longer than 2 sentences.\"\n",
    "tone_instructions = \"Answer in a friendly and formal manner.\"\n",
    "\n",
    "instruction_text = f\"We are currently on {current_lecture}. {instruction_for_search}. {length_limitation}. {tone_instructions}\"\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=instruction_text,\n",
    "        event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()"
   ],
   "id": "77c4c2effbfa5928",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > file_search\n",
      "\n",
      "\n",
      "assistant > The concept of functions is out of the scope of this class, as it will be explained in Lecture 2. We will cover what a function is, how to recognize one, and function notation in that lecture[0].\n",
      "[0] Lecture02_Script.pdf\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# All Combined",
   "id": "62ee9943863889d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math professor\",\n",
    "    instructions=\"You are an Math professor. Use you knowledge base to answer questions about math lectures based on the provided files.\",\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")\n",
    "\n",
    "vector_store = client.beta.vector_stores.create(name=\"Math 201 lecture\")\n",
    "\n",
    "# prepare the files for upload to OpenAI\n",
    "file_paths = [\"C:\\projects\\Sports-Buddy\\support_material\\Lecture01_Script.pdf\",\n",
    "              \"C:\\projects\\Sports-Buddy\\support_material\\Lecture02_Script.pdf\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store, and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)\n",
    "\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        print(\"\\n\".join(citations))\n",
    "\n",
    "\n",
    "student_question = \"Can you explain what functions are?\"\n",
    "\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": student_question}],\n",
    "    tool_resources={\n",
    "        \"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store.id]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "current_lecture = \"Lecture 1\"\n",
    "instruction_for_search = \"Search in which lecture the requested concept is explained. If it appears on another Lecture that in not the current lecture, reply explaining that concept is out of the scope of this class (in a friendly manner), since it will be explained in the lecture X, where X is the lecture where it appears.\"\n",
    "length_limitation = \"Keep your answers short, no longer than 2 sentences.\"\n",
    "tone_instructions = \"Answer in a friendly and formal manner.\"\n",
    "\n",
    "instruction_text = f\"We are currently on {current_lecture}. {instruction_for_search}. {length_limitation}. {tone_instructions}\"\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=instruction_text,\n",
    "        event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()"
   ],
   "id": "28f1418c4bba51e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
