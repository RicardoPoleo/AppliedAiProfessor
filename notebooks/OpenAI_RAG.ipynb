{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-16T18:29:22.049277Z",
     "start_time": "2024-08-16T18:29:22.029947Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:29:25.975012Z",
     "start_time": "2024-08-16T18:29:24.253461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math professor\",\n",
    "    instructions=\"You are an Math professor. Use you knowledge base to answer questions about math lectures based on the provided files.\",\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ],
   "id": "fd68b7c161c51545",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:30:16.361690Z",
     "start_time": "2024-08-16T18:30:13.219233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_store = client.beta.vector_stores.create(name=\"Math 201 lecture\")\n",
    "\n",
    "# prepare the files for upload to OpenAI\n",
    "file_paths = [\"C:\\projects\\Sports-Buddy\\support_material\\Lecture01_Script.pdf\",\n",
    "              \"C:\\projects\\Sports-Buddy\\support_material\\Lecture02_Script.pdf\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store, and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ],
   "id": "52b170ad41e539b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=2, failed=0, in_progress=0, total=2)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:30:27.049011Z",
     "start_time": "2024-08-16T18:30:22.397146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        print(\"\\n\".join(citations))\n",
    "\n",
    "\n",
    "student_question = \"Can you explain what functions are?\"\n",
    "\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": student_question}],\n",
    "    tool_resources={\n",
    "        \"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store.id]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "current_lecture = \"Lecture 1\"\n",
    "instruction_for_search = \"Search in which lecture the requested concept is explained. If it appears on another Lecture that in not the current lecture, reply explaining that concept is out of the scope of this class (in a friendly manner), since it will be explained in the lecture X, where X is the lecture where it appears.\"\n",
    "length_limitation = \"Keep your answers short, no longer than 2 sentences.\"\n",
    "tone_instructions = \"Answer in a friendly and formal manner.\"\n",
    "\n",
    "instruction_text = f\"We are currently on {current_lecture}. {instruction_for_search}. {length_limitation}. {tone_instructions}\"\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=instruction_text,\n",
    "        event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()"
   ],
   "id": "77c4c2effbfa5928",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > file_search\n",
      "\n",
      "\n",
      "assistant > The concept of functions is out of the scope of this class, as it will be explained in Lecture 2. We will cover what a function is, how to recognize one, and function notation in that lecture[0].\n",
      "[0] Lecture02_Script.pdf\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# All Combined",
   "id": "62ee9943863889d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:50:19.119770Z",
     "start_time": "2024-08-16T18:49:59.254505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math professor\",\n",
    "    instructions=\"You are an Math professor. Use you knowledge base to answer questions about math lectures based on the provided files.\",\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")\n",
    "\n",
    "vector_store = client.beta.vector_stores.create(name=\"Math 201 lecture\")\n",
    "\n",
    "# prepare the files for upload to OpenAI\n",
    "file_paths = [\"C:\\projects\\Sports-Buddy\\support_material\\Lecture01_Script.pdf\",\n",
    "              \"C:\\projects\\Sports-Buddy\\support_material\\Lecture02_Script.pdf\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store, and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)\n",
    "\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        print(\"\\n\".join(citations))\n",
    "\n",
    "\n",
    "student_question = \"Can you explain what a rolling coaster is?\"\n",
    "\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": student_question}],\n",
    "    tool_resources={\n",
    "        \"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store.id]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "current_lecture = \"Lecture 1\"\n",
    "instruction_for_search = \"Search in which lecture the requested concept is explained. If it appears on another Lecture that in not the current lecture, reply explaining that concept is out of the scope of this class (in a friendly manner), since it will be explained in the lecture X, where X is the lecture where it appears.\"\n",
    "length_limitation = \"Keep your answers short, no longer than 2 sentences.\"\n",
    "tone_instructions = \"Answer in a friendly and formal manner.\"\n",
    "\n",
    "instruction_text = f\"We are currently on {current_lecture}. {instruction_for_search}. {length_limitation}. {tone_instructions}\"\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=instruction_text,\n",
    "        event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()"
   ],
   "id": "28f1418c4bba51e2",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\openai\\_base_client.py:1025\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1024\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1025\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[0;32m   1026\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m err:  \u001B[38;5;66;03m# thrown on 4xx and 5xx status code\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\httpx\\_models.py:761\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    760\u001B[0m message \u001B[38;5;241m=\u001B[39m message\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m, error_type\u001B[38;5;241m=\u001B[39merror_type)\n\u001B[1;32m--> 761\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HTTPStatusError(message, request\u001B[38;5;241m=\u001B[39mrequest, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPStatusError\u001B[0m: Server error '500 Internal Server Error' for url 'https://api.openai.com/v1/assistants'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\openai\\_base_client.py:1025\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1024\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1025\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[0;32m   1026\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m err:  \u001B[38;5;66;03m# thrown on 4xx and 5xx status code\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\httpx\\_models.py:761\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    760\u001B[0m message \u001B[38;5;241m=\u001B[39m message\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m, error_type\u001B[38;5;241m=\u001B[39merror_type)\n\u001B[1;32m--> 761\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HTTPStatusError(message, request\u001B[38;5;241m=\u001B[39mrequest, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPStatusError\u001B[0m: Server error '500 Internal Server Error' for url 'https://api.openai.com/v1/assistants'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 11\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopenai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenAI\n\u001B[0;32m      9\u001B[0m client \u001B[38;5;241m=\u001B[39m OpenAI(api_key\u001B[38;5;241m=\u001B[39mOPENAI_API_KEY)\n\u001B[1;32m---> 11\u001B[0m assistant \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mbeta\u001B[38;5;241m.\u001B[39massistants\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[0;32m     12\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMath professor\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     13\u001B[0m     instructions\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are an Math professor. Use you knowledge base to answer questions about math lectures based on the provided files.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     14\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-4o\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     15\u001B[0m     tools\u001B[38;5;241m=\u001B[39m[{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile_search\u001B[39m\u001B[38;5;124m\"\u001B[39m}],\n\u001B[0;32m     16\u001B[0m )\n\u001B[0;32m     18\u001B[0m vector_store \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mbeta\u001B[38;5;241m.\u001B[39mvector_stores\u001B[38;5;241m.\u001B[39mcreate(name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMath 201 lecture\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# prepare the files for upload to OpenAI\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\openai\\resources\\beta\\assistants.py:158\u001B[0m, in \u001B[0;36mAssistants.create\u001B[1;34m(self, model, description, instructions, metadata, name, response_format, temperature, tool_resources, tools, top_p, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;124;03mCreate an assistant with a model and instructions.\u001B[39;00m\n\u001B[0;32m     94\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001B[39;00m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m extra_headers \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpenAI-Beta\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistants=v2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(extra_headers \u001B[38;5;129;01mor\u001B[39;00m {})}\n\u001B[1;32m--> 158\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[0;32m    159\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/assistants\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    160\u001B[0m     body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[0;32m    161\u001B[0m         {\n\u001B[0;32m    162\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[0;32m    163\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdescription\u001B[39m\u001B[38;5;124m\"\u001B[39m: description,\n\u001B[0;32m    164\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstructions\u001B[39m\u001B[38;5;124m\"\u001B[39m: instructions,\n\u001B[0;32m    165\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[0;32m    166\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: name,\n\u001B[0;32m    167\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n\u001B[0;32m    168\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[0;32m    169\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_resources\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_resources,\n\u001B[0;32m    170\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[0;32m    171\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[0;32m    172\u001B[0m         },\n\u001B[0;32m    173\u001B[0m         assistant_create_params\u001B[38;5;241m.\u001B[39mAssistantCreateParams,\n\u001B[0;32m    174\u001B[0m     ),\n\u001B[0;32m    175\u001B[0m     options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[0;32m    176\u001B[0m         extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[0;32m    177\u001B[0m     ),\n\u001B[0;32m    178\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mAssistant,\n\u001B[0;32m    179\u001B[0m )\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\openai\\_base_client.py:1266\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1252\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1253\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1254\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1261\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1262\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1263\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1264\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1265\u001B[0m     )\n\u001B[1;32m-> 1266\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\openai\\_base_client.py:942\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    933\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    934\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    935\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    940\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    941\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 942\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[0;32m    943\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m    944\u001B[0m         options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m    945\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[0;32m    946\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m    947\u001B[0m         remaining_retries\u001B[38;5;241m=\u001B[39mremaining_retries,\n\u001B[0;32m    948\u001B[0m     )\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\openai\\_base_client.py:1031\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[0;32m   1030\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m-> 1031\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_request(\n\u001B[0;32m   1032\u001B[0m         input_options,\n\u001B[0;32m   1033\u001B[0m         cast_to,\n\u001B[0;32m   1034\u001B[0m         retries,\n\u001B[0;32m   1035\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m   1036\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[0;32m   1037\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m   1038\u001B[0m     )\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\openai\\_base_client.py:1079\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1075\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[0;32m   1076\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m   1077\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m-> 1079\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[0;32m   1080\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m   1081\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1082\u001B[0m     remaining_retries\u001B[38;5;241m=\u001B[39mremaining,\n\u001B[0;32m   1083\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[0;32m   1084\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m   1085\u001B[0m )\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\openai\\_base_client.py:1031\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[0;32m   1030\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m-> 1031\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_request(\n\u001B[0;32m   1032\u001B[0m         input_options,\n\u001B[0;32m   1033\u001B[0m         cast_to,\n\u001B[0;32m   1034\u001B[0m         retries,\n\u001B[0;32m   1035\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m   1036\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[0;32m   1037\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m   1038\u001B[0m     )\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\RealTimeClass\\Lib\\site-packages\\openai\\_base_client.py:1077\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1073\u001B[0m log\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetrying request to \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m in \u001B[39m\u001B[38;5;132;01m%f\u001B[39;00m\u001B[38;5;124m seconds\u001B[39m\u001B[38;5;124m\"\u001B[39m, options\u001B[38;5;241m.\u001B[39murl, timeout)\n\u001B[0;32m   1075\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[0;32m   1076\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[1;32m-> 1077\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[0;32m   1079\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[0;32m   1080\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m   1081\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1084\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m   1085\u001B[0m )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
